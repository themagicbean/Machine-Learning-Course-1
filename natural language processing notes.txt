L189 NLP intuition
	how to clean text, bag of words model, apply ML to BOW
	
	wikipedia article re: history
		(transition from rule-based to stat-based via machines in 80s)
	
	Uses
		sentiment analysis
		predic genre of text
		question answering
		machine translation
		text summarization
		
	NLP libraries
		Natural Langauge Toolkit NLTK	nltk.org
		SpaCy
		Stanford NLP
		Open NLP
		
	NLTK uses
		POS (part of speech) tag
		visualize semantic relationships b/t words
		cluster similar words
		"bag of words" BOW model	
			preprocesses text before fitting
			vocab of known words + measure of presence of known words
			
L190 get dataset

L191 - Python NLP
	TSV v CSV file
		TSV > CSV because commas in reviews would be confusing
		(Would need to detab reviews though if TSV
		 though rare in html b/c tab is shortcut to next field)
		 
L198 what is BOW model? (also called vector space model)
	each review becomes a sparse matrix w/ number of occurences and words 
	(wikipedia has article)
	(sparse matrix = matrix with lots of 0s)
	ind vars are corpus, one ind var for each word (tokenized) (matrix of features)
	dependent variable vector is 1 / 0 positive / negative
	scikit token_pattern as alternative to help clean text (& other params in count vectorizer)
	
L199 - try to reduce sparsity 

L 201 homework
1. Run the other classification models we made in Part 3 - Classification, other than the one we used in the last tutorial.

2. Evaluate the performance of each of these models. Try to beat the Accuracy obtained in the tutorial. But remember, Accuracy is not enough, so you should also look at other performance metrics like Precision (measuring exactness), Recall (measuring completeness) and the F1 Score (compromise between Precision and Recall). Please find below these metrics formulas (TP = # True Positives, TN = # True Negatives, FP = # False Positives, FN = # False Negatives):

Accuracy = (TP + TN) / (TP + TN + FP + FN)

Precision = TP / (TP + FP)

Recall = TP / (TP + FN)

F1 Score = 2 * Precision * Recall / (Precision + Recall)

3. Try even other classification models that we haven't covered in Part 3 - Classification. Good ones for NLP include:

    CART
    C5.0
    Maximum Entropy

Submit your results in the Q&A for this Lecture or by pm and justify in few words why you think it's the most appropriate model.
	
	
--had error in DT copypasta


L 204
		convert all to lower to avoid duplicate entries for cap and lower	
			e.g., Pasta and pasta

			
Q&A
	stopwords includes negation words, which results in sentiment reversal
	n-gramming (combining words into tuples, etc) can fix per TA but no detail
	
	
L205-211 = R

L212: challenge to use other models and/or compare facets (results) of modles)

* need to get better at function /file management to automate
